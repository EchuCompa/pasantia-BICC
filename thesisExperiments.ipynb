{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asvFormula.experiments as exp\n",
    "from asvFormula.bayesianNetworks import networkSamplesPath\n",
    "from asvFormula import asvRunResultsPath\n",
    "from asvFormula.datasetManipulation import *\n",
    "from asvFormula.digraph import hasUnderlyingTree\n",
    "from pgmpy.readwrite import BIFReader\n",
    "from pgmpy.inference import VariableElimination\n",
    "\n",
    "from importlib import reload \n",
    "import asvFormula\n",
    "import asvFormula.experiments as exp\n",
    "\n",
    "exp = reload(asvFormula.experiments)\n",
    "\n",
    "#TODO: Fix the seed and the random so that I can reproduce the results. Right now it's not changing anything the seed method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running ASV for Cancer bayesian network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c6834a68fd144e29e7efbfa1aeb3f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model accuracy is : 0.8925\n"
     ]
    }
   ],
   "source": [
    "cancerNetworkPath = networkSamplesPath + \"/cancer.bif\"\n",
    "BNmodel = BIFReader(cancerNetworkPath).get_model()\n",
    "variableToPredict = \"Pollution\"\n",
    "numberOfSamples = 2000\n",
    "treeMaxDepth = 5\n",
    "\n",
    "BNInference, valuesPerFeature, encodedDataset, dtTreeClassifier, dtAsNetwork = initializeDataAndRemoveVariable(BNmodel, variableToPredict, numberOfSamples, treeMaxDepth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean prediction of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean prediction of model for the variable Pollution\n",
      "Mean prediction value for the decision tree: [0.00203   0.9986805], it took 0.008713950001038029 seconds\n",
      "Mean prediction value for the probabilities of the decision tree: [0.09486323 0.90513677], it took 0.008713950001038029 seconds\n",
      "Mean prediction value for possible values of the dataset: [0.007350000000000002, 0.9926500000000001], it took 0.015301499999623047 seconds\n",
      "Estimated value for shap explainer: [0.09625 0.90375]\n",
      "Probabilities of the variable in the bayesian network: [0.1, 0.9]\n"
     ]
    }
   ],
   "source": [
    "dataNoPrediction = encodedDataset.drop(variableToPredict, axis=1)\n",
    "first_instance = dataNoPrediction.iloc[0]\n",
    "\n",
    "asvCalc = exp.ASV(BNmodel, dtTreeClassifier, BNInference, valuesPerFeature, variableToPredict, 'Exact', first_instance)\n",
    "\n",
    "# Use the complete bayesian network to calculate the mean prediction\n",
    "completeBNModel = BIFReader(cancerNetworkPath).get_model()\n",
    "num_variables = len(completeBNModel.nodes())\n",
    "\n",
    "completeBNInference = VariableElimination(completeBNModel)\n",
    "\n",
    "exp.showMeanPredictionOfModel(variableToPredict, completeBNInference, valuesPerFeature, dtTreeClassifier, asvCalc, num_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running ASV for Child bayesian network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0937be787eda42fea1a6aa0d91df3ded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pgmpy:Probability values don't exactly sum to 1. Differ by: -2.220446049250313e-16. Adjusting values.\n",
      "WARNING:pgmpy:Probability values don't exactly sum to 1. Differ by: 3.3306690738754696e-16. Adjusting values.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model accuracy is : 0.68\n"
     ]
    }
   ],
   "source": [
    "childNetworkPath = networkSamplesPath + \"/child.bif\"\n",
    "\n",
    "def childBNAsTree(childNetworkPath):\n",
    "    treeBNChild = BIFReader(childNetworkPath).get_model()\n",
    "\n",
    "    #I remove this edges so that it is a tree and we can work with it\n",
    "    removeEdgeAndMarginalizeCPD(treeBNChild, 'LungParench', 'Grunting')\n",
    "    removeEdgeAndMarginalizeCPD(treeBNChild, 'LungParench', 'HypoxiaInO2')\n",
    "    removeEdgeAndMarginalizeCPD(treeBNChild, 'HypoxiaInO2', 'LowerBodyO2')\n",
    "    removeEdgeAndMarginalizeCPD(treeBNChild, 'CardiacMixing', 'HypDistrib')\n",
    "    removeEdgeAndMarginalizeCPD(treeBNChild, 'Sick', 'Age')\n",
    "    removeEdgeAndMarginalizeCPD(treeBNChild, 'LungFlow', 'ChestXray')\n",
    "\n",
    "    assert hasUnderlyingTree(treeBNChild)\n",
    "    return treeBNChild\n",
    "\n",
    "treeBNChild = childBNAsTree(childNetworkPath)\n",
    "variableToPredict = \"Age\"\n",
    "numberOfSamples = 10000\n",
    "treeMaxDepth = 7\n",
    "\n",
    "BNInference, valuesPerFeature, encodedDataset, dtTreeClassifier, dtAsNetwork = initializeDataAndRemoveVariable(treeBNChild, variableToPredict, numberOfSamples, treeMaxDepth)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean prediction of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean prediction of model for the variable Age\n",
      "Mean prediction value for the decision tree: [0.99781561 0.         0.00218439], it took 0.025556358999892836 seconds\n",
      "Mean prediction value for the probabilities of the decision tree: [0.76774309 0.06836589 0.16389102], it took 0.025556358999892836 seconds\n",
      "Mean prediction value for possible values of the dataset: [0.9947846567967703, 0.0, 0.005215343203230149], it took 380.02609960599875 seconds\n",
      "Estimated value for shap explainer: [0.691125 0.13775  0.171125]\n",
      "Probabilities of the variable in the bayesian network: [0.6489918355500001, 0.17148510286900004, 0.17952306158100004]\n"
     ]
    }
   ],
   "source": [
    "dataNoPrediction = encodedDataset.drop(variableToPredict, axis=1)\n",
    "first_instance = dataNoPrediction.iloc[0]\n",
    "\n",
    "asvCalc = exp.ASV(treeBNChild, dtTreeClassifier, BNInference, valuesPerFeature, variableToPredict, 'Exact', first_instance)\n",
    "\n",
    "# Use the complete bayesian network to calculate the mean prediction\n",
    "completeBNModel = BIFReader(childNetworkPath).get_model()\n",
    "completeBNInference = VariableElimination(completeBNModel)\n",
    "num_variables = len(completeBNModel.nodes())\n",
    "\n",
    "exp.showMeanPredictionOfModel(variableToPredict, completeBNInference, valuesPerFeature, dtTreeClassifier, asvCalc, 11)\n",
    "\n",
    "#The prediction for the first instance is 0, so if a lot of the features are fixed, then there is a higher chance that the prediction is 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ASV + Shapley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DecisionTreeDigraph' object has no attribute 'nodeMeanPrediction'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m dataNoPrediction \u001b[38;5;241m=\u001b[39m encodedDataset\u001b[38;5;241m.\u001b[39mdrop(variableToPredict, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m first_instance \u001b[38;5;241m=\u001b[39m dataNoPrediction\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteASVAndShapleyIntoFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst_instance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataNoPrediction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtTreeClassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43masvCalc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43masvRunResultsPath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/childASVAndShapleyMean.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvaluesPerFeature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariableToPredict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tesis/pasantia-BICC/asvFormula/experiments.py:76\u001b[0m, in \u001b[0;36mwriteASVAndShapleyIntoFile\u001b[0;34m(first_instance, features, dtTreeClassifier, asv, file, valuesPerFeature, variableToPredict, progress)\u001b[0m\n\u001b[1;32m     74\u001b[0m sumOfAsv \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(dtTreeClassifier\u001b[38;5;241m.\u001b[39mn_classes_)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, feature \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(features):\n\u001b[0;32m---> 76\u001b[0m     asvValue \u001b[38;5;241m=\u001b[39m \u001b[43masv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masvForFeature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfirst_instance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshowProgress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     sumOfAsv \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m asvValue\n\u001b[1;32m     78\u001b[0m     shapleyValue \u001b[38;5;241m=\u001b[39m shap_values[i]\n",
      "File \u001b[0;32m~/tesis/pasantia-BICC/asvFormula/exactASV.py:38\u001b[0m, in \u001b[0;36mASV.asvForFeature\u001b[0;34m(self, feature, instance, showProgress)\u001b[0m\n\u001b[1;32m     36\u001b[0m     fixedFeatures \u001b[38;5;241m=\u001b[39m classFeaturesOrder[:featureIndex]\n\u001b[1;32m     37\u001b[0m     fixedFeaturesWithFeature \u001b[38;5;241m=\u001b[39m classFeaturesOrder[:featureIndex\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 38\u001b[0m     asvValue \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m classSize \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeanPredictionForEquivalenceClass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfixedFeatures\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m  \n\u001b[1;32m     39\u001b[0m                             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeanPredictionForEquivalenceClass(fixedFeaturesWithFeature))\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m showProgress: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProgress of classes processed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(equivalenceClass)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m totalTopologicalOrders \u001b[38;5;241m==\u001b[39m allTopologicalOrders, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe total number of topological orders for the equivalence classes is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotalTopologicalOrders\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and the total number of topological orders for the graph is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mallTopologicalOrders\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/tesis/pasantia-BICC/asvFormula/exactASV.py:47\u001b[0m, in \u001b[0;36mASV.meanPredictionForEquivalenceClass\u001b[0;34m(self, fixedFeatures)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmeanPredictionForEquivalenceClass\u001b[39m(\u001b[38;5;28mself\u001b[39m, fixedFeatures : List[\u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmeanPredictionForDTinBNWithEvidence\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodelAsTree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrootNode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodelAsTree\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatureDistributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPathCondition\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvaluesPerFeature\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpriorEvidenceFrom\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfixedFeatures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnodePrediction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictionFunction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tesis/pasantia-BICC/asvFormula/bayesianNetworks/bayesianNetwork.py:145\u001b[0m, in \u001b[0;36mmeanPredictionForDTinBNWithEvidence\u001b[0;34m(decisionTreeGraph, node, bayesianNetwork, pathCondition, priorEvidence, nodePrediction)\u001b[0m\n\u001b[1;32m    142\u001b[0m previousLowerLimit \u001b[38;5;241m=\u001b[39m pathCondition\u001b[38;5;241m.\u001b[39mgetLowerLimit(feature)\n\u001b[1;32m    144\u001b[0m pathCondition\u001b[38;5;241m.\u001b[39msetVariableUpperLimit(feature, math\u001b[38;5;241m.\u001b[39mfloor(treshold))\n\u001b[0;32m--> 145\u001b[0m leftMean \u001b[38;5;241m=\u001b[39m \u001b[43mmeanPredictionForDTinBNWithEvidence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecisionTreeGraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchildren\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbayesianNetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpathCondition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpriorEvidence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnodePrediction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m pathCondition\u001b[38;5;241m.\u001b[39msetVariableUpperLimit(feature, previousUpperLimit) \n\u001b[1;32m    148\u001b[0m pathCondition\u001b[38;5;241m.\u001b[39msetVariableLowerLimit(feature, math\u001b[38;5;241m.\u001b[39mceil(treshold))\n",
      "File \u001b[0;32m~/tesis/pasantia-BICC/asvFormula/bayesianNetworks/bayesianNetwork.py:131\u001b[0m, in \u001b[0;36mmeanPredictionForDTinBNWithEvidence\u001b[0;34m(decisionTreeGraph, node, bayesianNetwork, pathCondition, priorEvidence, nodePrediction)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmeanPredictionForDTinBNWithEvidence\u001b[39m(decisionTreeGraph : DecisionTreeDigraph, node, bayesianNetwork : VariableElimination, pathCondition : PathCondition, priorEvidence : \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mlist\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, nodePrediction : Callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m n, tree : tree\u001b[38;5;241m.\u001b[39mnodePrediction(n)) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pathCondition\u001b[38;5;241m.\u001b[39mdoesNotMatchEvidence(priorEvidence): \u001b[38;5;66;03m#It would be more efficient to only do this check when you add the variable to the path condition, but it's more legible this way (and the perfomance loss is not significant)\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m         possibleOutputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mnodePrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecisionTreeGraph\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mzeros(possibleOutputs)\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m isLeaf(node, decisionTreeGraph):\n",
      "File \u001b[0;32m~/tesis/pasantia-BICC/asvFormula/exactASV.py:14\u001b[0m, in \u001b[0;36mASV.__init__.<locals>.<lambda>\u001b[0;34m(n, tree)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatureDistributions \u001b[38;5;241m=\u001b[39m featureDistributions\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvaluesPerFeature \u001b[38;5;241m=\u001b[39m valuesPerFeature\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictionFunction \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mlambda\u001b[39;00m n, tree : \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnodeMeanPrediction\u001b[49m(n)) \u001b[38;5;28;01mif\u001b[39;00m predictionFunction \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMean\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m n, tree : tree\u001b[38;5;241m.\u001b[39mnodePrediction(n)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstance \u001b[38;5;241m=\u001b[39m instance\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatureToPredict \u001b[38;5;241m=\u001b[39m featureToPredict\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DecisionTreeDigraph' object has no attribute 'nodeMeanPrediction'"
     ]
    }
   ],
   "source": [
    "asvCalc = exp.ASV(treeBNChild, dtTreeClassifier, BNInference, valuesPerFeature, variableToPredict, predictionFunction = 'Mean')\n",
    "\n",
    "dataNoPrediction = encodedDataset.drop(variableToPredict, axis=1)\n",
    "first_instance = dataNoPrediction.iloc[0]\n",
    "\n",
    "exp.writeASVAndShapleyIntoFile(first_instance, list(dataNoPrediction.columns), dtTreeClassifier, asvCalc, asvRunResultsPath + \"/childASVAndShapleyMean.csv\", valuesPerFeature, variableToPredict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asvCalc = exp.ASV(treeBNChild, dtTreeClassifier, BNInference, valuesPerFeature, variableToPredict, predictionFunction='Exact')\n",
    "\n",
    "dataNoPrediction = encodedDataset.drop(variableToPredict, axis=1)\n",
    "first_instance = dataNoPrediction.iloc[0]\n",
    "\n",
    "exp.writeASVAndShapleeyIntoFile(first_instance, list(dataNoPrediction.columns), dtTreeClassifier, asvCalc, asvRunResultsPath + \"/childASVAndShapleyExact.csv\", valuesPerFeature, variableToPredict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the complete bayesian network to calculate the mean prediction\n",
    "completeBNModel = BIFReader(childNetworkPath).get_model()\n",
    "completeBNInference = VariableElimination(completeBNModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.showMeanPredictionOfModel(variableToPredict, completeBNInference, valuesPerFeature, dtTreeClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtAsNetwork = exp.obtainDecisionTreeDigraph(dtTreeClassifier, dataNoPrediction.columns)\n",
    "#exp.drawDecisionTree(dtAsNetwork)\n",
    "#exp.drawGraph(treeBNChild)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
