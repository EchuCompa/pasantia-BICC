{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  asvFormula.classesSizes.recursiveFormula import *\n",
    "from asvFormula.bayesianNetworks.bayesianNetwork import *\n",
    "from asvFormula.bayesianNetworks import networkSamplesPath\n",
    "from asvFormula.datasetManipulation import *\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import random \n",
    "from pgmpy.readwrite import BIFReader\n",
    "from pgmpy.inference import VariableElimination\n",
    "import random\n",
    "import sys,os\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "printEnabled = True\n",
    "def disablePrint():\n",
    "    global printEnabled\n",
    "    if printEnabled:\n",
    "        sys._jupyter_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "        printEnabled = False\n",
    "\n",
    "def enablePrint():\n",
    "    global printEnabled\n",
    "    printEnabled = True\n",
    "    sys.stdout.close()\n",
    "    sys.stdout = sys._jupyter_stdout\n",
    "\n",
    "def convertDictToCsv(dict, filename):\n",
    "\n",
    "    df = pd.DataFrame.from_dict(dict, orient='index')\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running ASV in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb9ea4f389d43cdb66b6cac44d29d99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random.seed(10)\n",
    "\n",
    "cancerNetworkPath = networkSamplesPath + \"/cancer.bif\"\n",
    "cancerNetworkPath = networkSamplesPath + \"/cancer_tree.bif\"\n",
    "\n",
    "BNmodel = BIFReader(cancerNetworkPath).get_model()\n",
    "BNInference = VariableElimination(BNmodel)\n",
    "\n",
    "variableToPredict = \"Cancer\"\n",
    "treeMaxDepth = 2\n",
    "# Create a BNDatabaseGenerator object from the model\n",
    "dataFromBN = datasetFromBayesianNetwork(BNmodel, 2000)\n",
    "featureColumns = dataFromBN.columns\n",
    "valuesPerFeature, encodedDataset = encodeCategoricalColumns(dataFromBN)\n",
    "dtTreeClassifier = decisionTreeFromDataset(encodedDataset, variableToPredict , treeMaxDepth)\n",
    "\n",
    "dtAsNetwork = obtainNetworkXTreeStructure(dtTreeClassifier, featureColumns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the Shapley Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Initialize the TreeExplainer\n",
    "explainer = shap.TreeExplainer(dtTreeClassifier)\n",
    "\n",
    "# Compute Shapley values for the test set\n",
    "#shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# For binary classification, shap_values returns a list with two arrays\n",
    "# We'll use the values corresponding to the positive class (income >50K)\n",
    "#shap_values = shap_values[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ASV Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asvForFeature(dag : nx.DiGraph, feature : str, instance : pd.Series, model, dataset : pd.DataFrame, feature_distributions : VariableElimination) -> float:\n",
    "\n",
    "    equivalenceClasses = equivalenceClassesFor(dag, feature)\n",
    "    asvValue = 0\n",
    "    for equivalenceClass in equivalenceClasses:\n",
    "        classFeaturesOrder = equivalenceClass[0]\n",
    "        classSize = equivalenceClass[1]\n",
    "        asvValue += classSize * asvForEquivalenceClass(classFeaturesOrder, feature, instance, model, dataset, feature_distributions)\n",
    "\n",
    "    return asvValue\n",
    "\n",
    "\n",
    "\n",
    "def asvForEquivalenceClass(classFeaturesOrder : List[str], feature : str, instance : pd.Series, model, dataset : pd.DataFrame, feature_distributions : VariableElimination) -> float:\n",
    "    asvValue = 0\n",
    "    \n",
    "    realFeatures = classFeaturesOrder[:classFeaturesOrder.index(feature)]\n",
    "    \n",
    "    for _, matchingInstance in matchingInstances(dataset, realFeatures, instance).iterrows():\n",
    "        asvValue += model.predict(matchingInstance.to_frame().T)[0] * probOfInstance(matchingInstance, realFeatures, feature_distributions)\n",
    "    return asvValue\n",
    "\n",
    "\n",
    "def matchingInstances(dataset, realFeatures, instance):\n",
    "    matchingInstances = dataset.copy()\n",
    "    for feature in realFeatures:\n",
    "        matchingInstances = matchingInstances[matchingInstances[feature] == instance[feature]]\n",
    "    return matchingInstances\n",
    "\n",
    "def probOfInstance(matchingInstance : pd.Series, realFeatures : List[str], feature_distributions : VariableElimination) -> float:\n",
    "    \n",
    "    decodedInstance = decodeInstance(matchingInstance, valuesPerFeature)\n",
    "    priorEvidence = {realFeature : decodedInstance[realFeature] for realFeature in realFeatures}\n",
    "    variablesToEstimate = [feature for feature in decodedInstance.keys() if feature not in realFeatures]\n",
    "    \n",
    "    inference = feature_distributions.query(variables=variablesToEstimate, evidence=priorEvidence)\n",
    "    \n",
    "    return inference.get_value(**{var : decodedInstance[var] for var in variablesToEstimate})\n",
    "\n",
    "def decodeInstance(instance : pd.Series, valuesPerFeature : dict[str, list]) -> pd.Series:\n",
    "    decodedInstance = instance.copy()\n",
    "    for feature in instance.keys():\n",
    "        if feature in valuesPerFeature:\n",
    "            decodedInstance[feature] = valuesPerFeature[feature][instance[feature]]\n",
    "    return decodedInstance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the ASV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = encodedDataset.drop(variableToPredict, axis=1) # Remove the target variable from the dataset\n",
    "asvForFeature(BNmodel, \"Xray\", data.iloc[0], dtTreeClassifier, data, BNInference)\n",
    "\n",
    "#I need to define what I will do if the variable that I want to predict is the one that is being used as evidence in the ASV calculation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
